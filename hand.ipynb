{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccaa6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand distance measurements can also be performed using object recognition techniques. Here's a concise overview of the process:\n",
    "\n",
    "# Data collection: Collect a dataset of images or videos that include hands at various distances from the camera. It's preferable to annotate the dataset with ground truth distance measurements.\n",
    "\n",
    "# Hand detection: Utilize a hand detection algorithm to locate and extract the hand region from the input images or video frames. This could involve using techniques like Haar cascades, HOG (Histogram of Oriented Gradients), or deep learning-based object detectors such as Faster R-CNN or YOLO.\n",
    "\n",
    "# Feature extraction: Extract relevant features from the detected hand region that can help estimate the distance. These features may include hand size, shape, texture, or specific keypoints.\n",
    "\n",
    "# Distance estimation: Develop a regression model or use existing techniques to estimate the distance based on the extracted features. This could involve techniques like linear regression, support vector regression, or random forest regression.\n",
    "\n",
    "# Training and evaluation: Split the dataset into training and validation sets. Train the regression model using the training data and evaluate its performance on the validation set. Measure the accuracy of the distance predictions using appropriate metrics.\n",
    "\n",
    "# Deployment: After training and evaluation, the trained regression model can be deployed to estimate hand distances in real-time applications. Given a hand region, extract the relevant features and feed them into the model to obtain the predicted distance.\n",
    "\n",
    "# Using object recognition for hand distance measurement allows for more flexibility, as it does not rely on a specific camera setup or depth sensing technology. However, it might not be as accurate as depth-based methods or may require additional calibration to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09eff15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed317d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hand Tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Open the video capture\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for webcam or provide a video file path\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Hand Tracking\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get the landmark positions for the hand\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                # Access specific landmark positions (e.g., index finger tip)\n",
    "                x = int(landmark.x * frame.shape[1])\n",
    "                y = int(landmark.y * frame.shape[0])\n",
    "                z = landmark.z  # Depth value (relative to camera)\n",
    "\n",
    "                # Do something with the landmark positions and depth value\n",
    "\n",
    "    # Display the frame with hand landmarks\n",
    "    cv2.imshow('Hand Landmarks', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# In this code, we use the MediaPipe library to detect and track hands in real-time. We initialize the mp_hands.Hands object with the desired configuration parameters such as max_num_hands (maximum number of hands to detect) and min_detection_confidence (minimum confidence threshold for detection).\n",
    "\n",
    "# Within the main loop, we read frames from the video source, convert them to RGB format (required by MediaPipe), and process the frames using hands.process(). The results obtained from hand tracking are stored in the results variable.\n",
    "\n",
    "# If hands are detected (results.multi_hand_landmarks), we iterate over the hand landmarks and access their positions using the landmark.x, landmark.y, and landmark.z attributes. The x and y values represent the 2D coordinates of the landmarks on the frame, while the z value represents the depth value (relative to the camera).\n",
    "\n",
    "# You can perform further calculations or use depth estimation techniques to estimate the hand distance based on the obtained landmarks and depth values.\n",
    "\n",
    "# The processed frames are displayed with hand landmarks, and the loop continues until 'q' is pressed to exit.\n",
    "\n",
    "# Make sure to have the MediaPipe and OpenCV libraries installed (pip install mediapipe opencv-python) before running the code.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
